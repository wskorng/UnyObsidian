この節ではまずtransformerを理解するのに必要なユニットを説明する。

## Attentionおさらい
Attentionはこうだった。
$$
\mathrm{output} = \mathrm{softmax}_{各行} ( \cfrac{1}{\sqrt{d}} Q K^\top ) V
$$
ただし$Q=EW^Q$、$K=DW^K$、$V=DW^V$で変換したもの。
![[attention cell.png]]
attentionっていいよね 文脈って感じのものを並列処理で取得できて

## Self-attention
attentionの入力として$E$も$D$も同じ$X$を使ったものをself-attentionという。
$$
\text{output} = \mathrm{softmax}_{各列} ( \cfrac{1}{\sqrt{d}} X W^Q W^{K\top} X^\top ) X W^V
$$
これ$W^Q W^{K\top} $って任意のrank $d$の行列ではあるな だからたぶん$d$を$l$より大きくするのはあんま意味ないよな

## Multi-head self-attention
self-attentionするにあたって、例えば単語の性と単語の意味と単語の場所なんて関係しないだろう。
そこで、inputをsliceし、それぞれを異なるパラメーターで独立にself-attentionし、concatしてoutputとしよう。
つまりこの独立なself-attentionがそれぞれ単語の性、単語の意味、単語の場所の関係を表現する、みたいなのを期待する。
これで計算量落とせるのはわかるね？俺はテンソルの計算量に詳しいから

## Feed-forward Network
Multi-head self-attentionは位置間の関係を捉えるのに集中させて、その後各位置ごとの処理を行う。
$$
\mathrm{FFN}(X) = \mathrm{ReLU}(X @ W_1 + B_1) @ W_2 + B_2
$$
中間層は$d$より大きくして豊富に表現するらしい。
まあ普通に1層半のDNNなんだけど、今回の文脈ではFFN(Feed-Forward Network)という。

## Masked multi-head self-attention
言語モデルの場合、$e_i$の出力には$e_{i-1}$までは参照するが$e_i$以降は参照しないのだが、単にMHSAとFFNを繰り返してるだけではそんなの表現されない。
そこでMHSAのsoftmax内に上三角部分が$-\infty$で他は$0$の行列$M$を足す。これでAttention matrix
$$
\mathrm{softmax}_{各列} ( \cfrac{1}{\sqrt{d}} X W^Q W^{K\top} X^\top + M )
$$
の上三角成分が0となり、
$$
\mathrm{softmax}_{各列} ( \cfrac{1}{\sqrt{d}} X W^Q W^{K\top} X^\top + M ) X
$$
の出力の$i$行目は$i$個の$l$次元ベクトル$\mathbf{x}^1,\cdots\mathbf{x}^i$の線形結合となる。もちろん
$$
\mathrm{softmax}_{各列} ( \cfrac{1}{\sqrt{d}} X W^Q W^{K\top} X^\top + M )
$$
の項は未来からの影響があるが、未来の$\mathbf{x}$に仮置きで「未定」を表す値を置いてもちゃんと出力ができるようにはなる。

## transformer
これらユニットをこうつなげる
![[transformer.avif]]
これがま～高性能なんだわ
Attention is all you need っていう論文で発表された革命的なモデルでtransformerといいます



## 参考


https://zenn.dev/kitacom_blog/articles/observe_self_attention_p2?redirected=1
https://zenn.dev/kitacom_blog/articles/observe_self_attention_p1?redirected=1
https://zenn.dev/kitacom_blog/articles/transformer_overall?redirected=1#%E3%82%A8%E3%83%B3%E3%82%B3%E3%83%BC%E3%83%80%E3%83%BC%E3%81%AF%E5%85%A5%E5%8A%9B%E6%96%87%E3%82%92%E6%A7%8B%E6%88%90%E3%81%99%E3%82%8B%E5%8D%98%E8%AA%9E%E3%82%92%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%81%AB%E5%A4%89%E6%8F%9B%E3%81%99%E3%82%8B
https://qiita.com/omiita/items/07e69aef6c156d23c538



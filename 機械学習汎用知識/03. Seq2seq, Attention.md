## Seq2seq
RNNをベースに翻訳をするネットワークを作ろう。
文全部読まずに翻訳はできないので、ReadingセルとWritingセルを用意し、Readingセルの最終状態をWritingセルの初期状態として、下図のようにするのがよいだろう。こういう形式のネットワークをSeq2seqという。
![[seq2seq.avif]]
もちろんLSTMくらいは使うのだが、長文になると最初の方の単語に誤訳が増えていく。

## Attention
これの解決策として、普通に人間が翻訳する時をヒントに考えよう。
人間が翻訳するとき、文全体を大まかに組んで、各単語を書くときは該当する単語をそのたび参照し確認するよね。この「該当する単語をそのたび参照し確認する」を再現するのがいいだろう。
encoderの各LSTMセルの出力$\mathbf{e^1},\mathbf{e^2},\mathbf{e^3},\mathbf{e^4},\mathbf{e^5}$、decoderの各LSTMセルの出力$\mathbf{d^1},\mathbf{d^2},\mathbf{d^3},\mathbf{d^4},\mathbf{d^5}$について考える(今後こいつらはサイズ$l$の行ベクトルでシーケンス長$5$)。

我々には経験があるためそういうのは$e$が正規直交基底をなしているなら
$$
\text{fixed} \,\mathbf{d}^j = \sum_i (\mathbf{d}^i \cdot \mathbf{e}^i) \mathbf{e}^j
$$
つまり$5\times l$行列でいえば
$$
\text{fixed} \ D = DE^\top E
$$
で済む話なのを知っている。

だが今回はぜんぜん正規直交基底ではないので、以下のようにする。
各LSTMセルの出力をある$d$次元の意味空間に落とし込む$l\times d$のパラメーター行列$W^K,W^V,W^Q$を作り、
$$
\begin{align*}
K=E W^K \\
V=E W^V\\
Q=D W^Q
\end{align*}
$$
としてこれら$5\times d$の行列をキー行列,バリュー行列,クエリ行列と呼び、
$$
\mathrm{softmax}_{各行} ( \cfrac{1}{\sqrt{n}} Q K^\top )
$$
をAttention matrixと呼び ($5 \times 5$) 、
$$
\mathrm{output} = \mathrm{softmax}_{各行} ( \cfrac{1}{\sqrt{n}} Q K^\top ) V
$$
という$5\times d$の出力を得る。計算グラフで表すならこう。
![[attention cell.png]]
これなら「クエリベクトル$\mathbf{q}^i$に対しキー行列$K$の各列との内積で合致度を計算し、softmaxで正規化してから、バリュー行列$V$の各列の線形結合を作って修正後クエリとする」というイメージで良い感じになりそう。
この機構をSeq2seqのどこに組み込むかはわかるから図はいらないよね。パラメーターは$W^K,W^V,W^Q$だけ増えたけど確かにそうした価値あるくらいに精度が上がる。
この機構をAttentionという。


## 参考
https://qiita.com/ta2bonn/items/c645ecbcf9dabd0c4778
https://zenn.dev/labbase/articles/cc458f9c6cafc7